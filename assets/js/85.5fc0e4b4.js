(window.webpackJsonp=window.webpackJsonp||[]).push([[85],{443:function(t,e,r){"use strict";r.r(e);var a=r(44),n=Object(a.a)({},(function(){var t=this,e=t.$createElement,r=t._self._c||e;return r("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[r("h1",{attrs:{id:"bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[t._v("#")]),t._v(" BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding")]),t._v(" "),r("blockquote",[r("p",[t._v("作者：  Jacob Devlin\tMing-Wei Chang\tKenton Lee\tKristina Toutanova")]),t._v(" "),r("p",[t._v("文献类型：期刊")]),t._v(" "),r("p",[t._v("期刊：arXiv")]),t._v(" "),r("p",[t._v("关键词：NLP，Transformer，Attention")])]),t._v(" "),r("h2",{attrs:{id:"研究内容"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#研究内容"}},[t._v("#")]),t._v(" 研究内容")]),t._v(" "),r("p",[t._v("BERT模型的目标通过对所有层的左右上下文进行联合调节，从"),r("strong",[t._v("未标记的文本")]),t._v("中预训练"),r("strong",[t._v("深度双向表示")]),t._v("，即：文本的语义表示，然后将文本的语义表示在特定NLP任务中作微调，最终应用于该NLP任务(BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers)")]),t._v(" "),r("p",[r("strong",[t._v("Bidirectional-双向")]),t._v("的意思表示它在处理一个词的时候，能考虑到该词前面和后面单词的信息，从而获取上下文的语义")]),t._v(" "),r("h3",{attrs:{id:"_1、bert的架构"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_1、bert的架构"}},[t._v("#")]),t._v(" 1、BERT的架构")]),t._v(" "),r("p",[t._v("BERT的模型架构基于了Transformer，实现了多层双向的Transformer编码器。文中有两个模型，一个是1.1亿参数的base模型，一个是3.4亿参数的large模型。里面所设置的参数如下：")]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",{staticStyle:{"text-align":"center"}},[t._v("Model")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Transformer层数(L)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("Hidden units(H)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("the number of self-attention heads(A)")]),t._v(" "),r("th",{staticStyle:{"text-align":"center"}},[t._v("总参数")])])]),t._v(" "),r("tbody",[r("tr",[r("td",{staticStyle:{"text-align":"center"}},[t._v("BERT(base)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("12")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("768")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("12")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1.1亿")])]),t._v(" "),r("tr",[r("td",{staticStyle:{"text-align":"center"}},[t._v("BERT(large)")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("24")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("1024")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("16")]),t._v(" "),r("td",{staticStyle:{"text-align":"center"}},[t._v("3.4亿")])])])]),t._v(" "),r("p",[t._v("其中base模型的参数和OpenAI的GPT的参数一致。目的就是为了同GPT的效果进行一个比较。")]),t._v(" "),r("h3",{attrs:{id:"_2、bert的输入表征"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_2、bert的输入表征"}},[t._v("#")]),t._v(" 2、BERT的输入表征")]),t._v(" "),r("img",{attrs:{src:t.$withBase("/paper/Bert/Figure 2.jpg"),alt:"mixureSecure"}}),t._v(" "),r("p",[t._v("主要由如下三部分组成：")]),t._v(" "),r("ul",[r("li",[r("strong",[t._v("Token Embeddings")]),t._v("：\n"),r("ul",[r("li",[t._v("第一个token始终是特殊的分类嵌入[CLS]\n"),r("ul",[r("li",[t._v("用作分类任务的聚合序列表示")]),t._v(" "),r("li",[t._v("非分类任务，则忽略")])])]),t._v(" "),r("li",[t._v("两个句子间会有一个分隔嵌入[SEP]")]),t._v(" "),r("li",[t._v("用##表示分词")])])]),t._v(" "),r("li",[r("strong",[t._v("Segment Embeddings")]),t._v("：对于同一个句子，使用相同的Embedding，如上图所示，“My dog is cute”用EA标记，“he likes play ##ing”用EB标记。")]),t._v(" "),r("li",[r("strong",[t._v("Position Embeddings")]),t._v("：\n"),r("ul",[r("li",[t._v("加入位置信息，支持序列的长度是512")])])])]),t._v(" "),r("p",[t._v("对于不同的NLP任务，模型输入会有"),r("strong",[t._v("微调")]),t._v("，对模型输出的利用也有差异，例如：")]),t._v(" "),r("ul",[r("li",[r("img",{attrs:{src:t.$withBase("/paper/Bert/Figure 4.jpg"),alt:"mixureSecure"}})]),t._v(" "),r("li",[r("p",[t._v("其中(a)和(b)是"),r("strong",[t._v("序列级任务")]),t._v("，而(c)和(d)是"),r("strong",[t._v("标记级任务")])])]),t._v(" "),r("li",[r("p",[t._v("语句对分类任务-图a&c")]),t._v(" "),r("ul",[r("li",[r("p",[t._v("该任务的实际应用场景包括："),r("strong",[t._v("问答")]),t._v("（判断一个问题与一个答案是否匹配）、"),r("strong",[t._v("语句匹配")]),t._v("（两句话是否表达同一个意思）等。对于该任务，BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分，如下图所示：")]),t._v(" "),r("img",{attrs:{src:t.$withBase("/paper/Bert/语句对分类任务.jpeg"),alt:"mixureSecure"}})])])]),t._v(" "),r("li",[r("p",[t._v("单文本分类任务-图b")]),t._v(" "),r("ul",[r("li",[r("p",[t._v("对于文本分类任务，BERT模型在文本前插入一个[CLS]符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如下图所示。可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。")]),t._v(" "),r("img",{attrs:{src:t.$withBase("/paper/Bert/单文本分类任务.jpeg"),alt:"mixureSecure"}})])])]),t._v(" "),r("li",[r("p",[t._v("序列标注任务-图d")]),t._v(" "),r("ul",[r("li",[r("p",[t._v("该任务的实际应用场景包括：中文分词&新词发现（标注每个字是词的首字、中间字或末字）、答案抽取（答案的起止位置）等。对于该任务，BERT模型利用文本中每个字对应的输出向量对该字进行标注（分类），如下图所示(B、I、E分别表示一个词的第一个字、中间字和最后一个字)。")]),t._v(" "),r("img",{attrs:{src:t.$withBase("/paper/Bert/序列标注任务.jpeg"),alt:"mixureSecure"}})])])])]),t._v(" "),r("h3",{attrs:{id:"_3、bert中核心内容-预训练"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#_3、bert中核心内容-预训练"}},[t._v("#")]),t._v(" 3、BERT中核心内容-预训练")]),t._v(" "),r("p",[t._v("BERT原文中提出两个非监督任务对BERT进行预训练："),r("strong",[t._v("Masked LM(MLM)"),r("strong",[t._v("和")]),t._v("Next Sentence Prediction(NSP)")])]),t._v(" "),r("h4",{attrs:{id:"task-1-masked-lm-mlm"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#task-1-masked-lm-mlm"}},[t._v("#")]),t._v(" "),r("strong",[t._v("Task#1：Masked LM(MLM)")])]),t._v(" "),r("ul",[r("li",[t._v("为了实现深度的双向表示，使得双向的作用让每个单词能够在多层上下文中间接的看到自己，因此采用了MLM")]),t._v(" "),r("li",[t._v("MLM的基本思路：随机（按一定比率）屏蔽（Mask）掉部分输入token，然后再去预测这些被屏蔽掉的token")]),t._v(" "),r("li",[r("img",{attrs:{src:t.$withBase("/paper/Bert/MLM.jpg"),alt:"mixureSecure"}})])]),t._v(" "),r("table",[r("thead",[r("tr",[r("th",[t._v("The training data generator chooses 15% of the token positions at random for prediction. If thei-th token is chosen, we replace thei-th token with :"),r("br"),t._v("(1) the [MASK] token 80% of the time"),r("br"),t._v("(2) a random token 10% of the time "),r("br"),t._v("(3) the unchangedi-th token 10% of the time.")])])]),t._v(" "),r("tbody",[r("tr",[r("td",[t._v("训练数据生成器随机选择 15% 的token位置来进行预测。 如果选择了第 i 个token，我们将第 i 个token替换为："),r("br"),t._v("(1) 80% 的情况下为 [MASK]"),r("br"),t._v("(2) 10% 的情况下为随机 "),r("br"),t._v("(3) 10% 的情况下为保持不变。")])])])]),t._v(" "),r("h4",{attrs:{id:"task-2-next-sentence-prediction-nsp"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#task-2-next-sentence-prediction-nsp"}},[t._v("#")]),t._v(" "),r("strong",[t._v("Task#2：Next Sentence Prediction(NSP)")])]),t._v(" "),r("ul",[r("li",[t._v("NSP的基本思路：当为每个预训练示例选择句子A和B时，50%的概率B是A之后的实际下一个句子(标记为IsNext)，50%的概率B是语料库中的随机句子(标记为NotNext)，"),r("strong",[t._v("判断句子B在文本中是否紧跟在句子A之后")]),t._v("。")]),t._v(" "),r("li",[r("img",{attrs:{src:t.$withBase("/paper/Bert/NSP.jpeg"),alt:"mixureSecure"}})])]),t._v(" "),r("h2",{attrs:{id:"创新点"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#创新点"}},[t._v("#")]),t._v(" 创新点")]),t._v(" "),r("ul",[r("li",[t._v("文章不使用传统的从左到右或从右到左的语言模型来预训练BERT。相反，使用两个新的无监督预测任务对BERT进行预训练")]),t._v(" "),r("li",[t._v("待补充，需要读完GPT和ELMo文献后再进行比较")])]),t._v(" "),r("h2",{attrs:{id:"参考链接"}},[r("a",{staticClass:"header-anchor",attrs:{href:"#参考链接"}},[t._v("#")]),t._v(" 参考链接")]),t._v(" "),r("ul",[r("li",[r("a",{attrs:{href:"https://cloud.tencent.com/developer/article/1389555",target:"_blank",rel:"noopener noreferrer"}},[t._v("图解BERT模型：从零开始构建BERT"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/311156298",target:"_blank",rel:"noopener noreferrer"}},[t._v("Transformer - Attention is all you need"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/410776234",target:"_blank",rel:"noopener noreferrer"}},[t._v("超详细图解Self-Attention"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/411311520",target:"_blank",rel:"noopener noreferrer"}},[t._v("熬了一晚上，我从零实现了Transformer模型，把代码讲给你听"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.cnblogs.com/gxcdream/p/7597865.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("向量内积（点乘）和外积（叉乘）概念及几何意义"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://www.jianshu.com/p/160c4800b9b5",target:"_blank",rel:"noopener noreferrer"}},[t._v("BERT模型学习与分析"),r("OutboundLink")],1)]),t._v(" "),r("li",[r("a",{attrs:{href:"https://zhuanlan.zhihu.com/p/95594311",target:"_blank",rel:"noopener noreferrer"}},[t._v("关于BERT的若干问题整理记录"),r("OutboundLink")],1)])])])}),[],!1,null,null,null);e.default=n.exports}}]);