(window.webpackJsonp=window.webpackJsonp||[]).push([[86],{441:function(t,n,e){"use strict";e.r(n);var r=e(44),a=Object(r.a)({},(function(){var t=this,n=t.$createElement,e=t._self._c||n;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"ernie-gram-pre-training-with-explicitly-n-gram-masked-language-modeling-for-natural-language-understanding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#ernie-gram-pre-training-with-explicitly-n-gram-masked-language-modeling-for-natural-language-understanding"}},[t._v("#")]),t._v(" ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding")]),t._v(" "),e("blockquote",[e("p",[t._v("作者：百度团队")]),t._v(" "),e("p",[t._v("文献类型：arXiv")]),t._v(" "),e("p",[t._v("关键词：自然语言处理")])]),t._v(" "),e("h2",{attrs:{id:"abstract"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#abstract"}},[t._v("#")]),t._v(" Abstract")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("英")]),t._v(" "),e("th",[t._v("汉")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[e("strong",[t._v("Coarse-grained")]),t._v(" linguistic information, such as named entities or phrases, facilitates "),e("strong",[t._v("adequately")]),t._v(" representation learning in pre-training.")]),t._v(" "),e("td",[e("strong",[t._v("粗粒度")]),t._v("的语言信息，例如命名实体或短语，有助于在预训练中"),e("strong",[t._v("充分地")]),t._v("进行表征学习。")])]),t._v(" "),e("tr",[e("td",[t._v("Previous works mainly focus on extending the objective of BERT’s Masked Language Modeling (MLM) from masking individual tokens to "),e("strong",[t._v("contiguous sequences")]),t._v(" of "),e("em",[t._v("n")]),t._v(" tokens.")]),t._v(" "),e("td",[t._v("以前的工作主要集中在将 BERT 的掩码语言建模 (MLM) 的目标从掩码单个token扩展到 n 个token的"),e("strong",[t._v("连续序列")]),t._v("。")])]),t._v(" "),e("tr",[e("td",[t._v("We argue that such contiguously masking method neglects to model the "),e("strong",[t._v("intra-dependencies")]),t._v(" and "),e("strong",[t._v("inter-relation")]),t._v(" of coarse-grained linguistic information.")]),t._v(" "),e("td",[t._v("我们认为，这种连续屏蔽方法忽略了对粗粒度语言信息的"),e("strong",[t._v("内部依赖性")]),t._v("和"),e("strong",[t._v("相互关系")]),t._v("进行建模。")])]),t._v(" "),e("tr",[e("td",[t._v("As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to "),e("strong",[t._v("enhance")]),t._v(" the integration of coarse-grained information into pre-training.")]),t._v(" "),e("td",[t._v("作为替代方案，我们提出了 ERNIE-Gram，这是一种明确的 n-gram 掩码方法，用于"),e("strong",[t._v("增强")]),t._v("粗粒度信息与预训练的集成。")])]),t._v(" "),e("tr",[e("td",[t._v("In ERNIE-Gram,"),e("em",[t._v("n")]),t._v("-grams "),e("strong",[t._v("are masked")]),t._v(" and predicted directly using explicit "),e("em",[t._v("n")]),t._v("-gram identities rather than contiguous sequences of "),e("em",[t._v("n")]),t._v(" tokens.")]),t._v(" "),e("td",[t._v("在ERNIE-Gram中，n-grams"),e("strong",[t._v("被屏蔽")]),t._v("，并直接使用明确的n-gram身份而不是n个tokens的连续序列来进行预测。")])]),t._v(" "),e("tr",[e("td",[t._v("Furthermore,ERNIE-Gram employs a "),e("strong",[t._v("generator model")]),t._v(" to sample plausible "),e("em",[t._v("n")]),t._v("-gram identities as optional n-gram masks and predict them in both coarse grained and fifine-grained manners to enable comprehensive "),e("em",[t._v("n")]),t._v("-gram prediction and relation modeling.")]),t._v(" "),e("td",[t._v("此外，ERNIE-Gram 使用"),e("strong",[t._v("生成器模型")]),t._v("对合理的 n-gram 身份进行采样作为可选的 n-gram 掩码，并以粗粒度和细粒度的方式预测它们，以实现全面的 n-gram 预测和关系建模。")])]),t._v(" "),e("tr",[e("td",[t._v("We pre-train ERNIE-Gram on English and Chinese "),e("strong",[t._v("text corpora")]),t._v(" and "),e("strong",[t._v("fifinetune")]),t._v(" on 19 "),e("strong",[t._v("downstream tasks")]),t._v(".")]),t._v(" "),e("td",[t._v("我们在英文和中文"),e("strong",[t._v("文本语料库")]),t._v("上预训练 ERNIE-Gram，并在 19 个"),e("strong",[t._v("下游任务")]),t._v("上进行"),e("strong",[t._v("微调")]),t._v("。")])]),t._v(" "),e("tr",[e("td",[t._v("Experimental results show that ERNIE-Gram "),e("strong",[t._v("outperforms")]),t._v(" previous pre-training models like "),e("strong",[t._v("XLNet")]),t._v(" and "),e("strong",[t._v("RoBERTa")]),t._v(" by a large margin, and achieves comparable results with "),e("strong",[t._v("state-of-the-art methods")]),t._v(".")]),t._v(" "),e("td",[t._v("实验结果表明，ERNIE-Gram 在很大程度上"),e("strong",[t._v("优于")]),t._v("先前的预训练模型，如 "),e("strong",[t._v("XLNet")]),t._v(" 和 "),e("strong",[t._v("RoBERTa")]),t._v("，并取得了与"),e("strong",[t._v("最先进方法")]),t._v("相当的结果。")])])])]),t._v(" "),e("h2",{attrs:{id:"_1-introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-introduction"}},[t._v("#")]),t._v(" 1 Introduction")]),t._v(" "),e("table",[e("thead",[e("tr",[e("th",[t._v("英")]),t._v(" "),e("th",[t._v("汉")])])]),t._v(" "),e("tbody",[e("tr",[e("td",[t._v("Pre-trained on "),e("strong",[t._v("large-scaled text corpora")]),t._v(" and fifinetuned on downstream tasks, "),e("strong",[t._v("self-supervised representation models")]),t._v(" have achieved remarkable improvements in "),e("strong",[t._v("natural language understanding (NLU)")]),t._v(".")]),t._v(" "),e("td",[t._v("在"),e("strong",[t._v("大规模文本语料库")]),t._v("上进行预训练并在下游任务上进行微调，"),e("strong",[t._v("自监督表示模型")]),t._v("在"),e("strong",[t._v("自然语言理解 (NLU)")]),t._v(" 方面取得了显着的进步。")])]),t._v(" "),e("tr",[e("td",[t._v("As one of the most prominent pre-trained models, BERT employs masked language modeling (MLM) to learn representations by masking individual tokens and predicting them based on their "),e("strong",[t._v("bidirectional context")]),t._v(".")]),t._v(" "),e("td",[t._v("作为最突出的预训练模型之一，BERT 采用掩码语言建模 (MLM) 来通过掩码单个token并根据其"),e("strong",[t._v("双向上下文")]),t._v("预测它们来学习表示。")])]),t._v(" "),e("tr",[e("td",[t._v("However, BERT’s MLM focuses on the represen tations of "),e("strong",[t._v("fifine-grained")]),t._v(" text units (e.g. words or subwords in English and characters in Chinese),rarely considering the "),e("strong",[t._v("coarse-grained")]),t._v(" linguistic information (e.g. named entities or phrases in English and words in Chinese) thus incurring inadequate representation learning.")]),t._v(" "),e("td",[t._v("然而，BERT 的 MLM 侧重于"),e("strong",[t._v("细粒度")]),t._v("文本单元（例如英文单词或子词和中文字符）的表示，很少考虑"),e("strong",[t._v("粗粒度")]),t._v("的语言信息（例如英文命名实体或短语和中文单词），因此导致表示学习不足。")])]),t._v(" "),e("tr",[e("td",[t._v("Many efforts have been devoted to integrate coarse-grained semantic information by "),e("strong",[t._v("independently masking and predicting")]),t._v(" contiguous sequences of n tokens, namely n-grams, such as named entities, phrases, whole words and text spans.")]),t._v(" "),e("td",[t._v("许多人致力于通过"),e("strong",[t._v("独立屏蔽和预测")]),t._v("n个 token的连续序列，即n-grams，如命名实体、短语、整个单词和文本跨度来整合粗粒度的语义信息。")])]),t._v(" "),e("tr",[e("td",[t._v("We argue that such contiguously masking strategies are "),e("strong",[t._v("less effective and reliable")]),t._v(" since the prediction of tokens in masked n-grams are independent of each other, which neglects the "),e("strong",[t._v("intra dependencies of n-grams")]),t._v(".")]),t._v(" "),e("td",[t._v("我们认为，这种连续掩蔽的策略"),e("strong",[t._v("不太有效和可靠")]),t._v("，因为被掩蔽的n-grams中的tokens的预测是相互独立的，这就忽略了"),e("strong",[t._v("n-grams的内部依赖关系")]),t._v("。")])]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td")]),t._v(" "),e("tr",[e("td"),t._v(" "),e("td")])])])])}),[],!1,null,null,null);n.default=a.exports}}]);