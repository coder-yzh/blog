(window.webpackJsonp=window.webpackJsonp||[]).push([[63],{418:function(s,t,a){"use strict";a.r(t);var n=a(44),e=Object(n.a)({},(function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h1",{attrs:{id:"bagging分类器"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#bagging分类器"}},[s._v("#")]),s._v(" Bagging分类器")]),s._v(" "),a("p",[s._v("Bagging算法（Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种"),a("strong",[s._v("团体学习算法")]),s._v("。最初由Leo Breiman于1996年提出。Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过"),a("strong",[s._v("降低结果的方差")]),s._v("，避免过拟合的发生，降低噪音数据的影响。又放回的重采样。")]),s._v(" "),a("p",[s._v("Bagging-Bootstrap 采样法的优点：原始数据中，仅仅约63%的样本被采样到。也就是说样本训练集中，约37%的错误训练集将被忽略掉，提高训练集的质量。")]),s._v(" "),a("p",[s._v("弱分类器，分类效果非常高。集成学习对流式数据进行分类，效果显著")]),s._v(" "),a("ul",[a("li",[s._v("弱分类器有差异性")]),s._v(" "),a("li",[s._v("弱分类器有相同见解")])]),s._v(" "),a("h4",{attrs:{id:"集成学习中单分类器的条件"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#集成学习中单分类器的条件"}},[s._v("#")]),s._v(" 集成学习中单分类器的条件")]),s._v(" "),a("p",[s._v("通过集成学习提高分类器的整体泛化能力是有条件的：")]),s._v(" "),a("ul",[a("li",[s._v("分类器之间应该具有差异性")]),s._v(" "),a("li",[s._v("分类器的精度，每个个体分类器的分类精度必须大于0.5")])]),s._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 导入相关库")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("ensemble "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" BaggingClassifier\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("neighbors "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" KNeighborsClassifier\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("model_selection "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" train_test_split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("metrics "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" accuracy_score\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("from")]),s._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("datasets "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" load_iris\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" np\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 下载数据集")]),s._v("\niris "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" load_iris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nX "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" iris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"data"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\ny "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" iris"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"target"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n\nnp"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("seed"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 划分数据集")]),s._v("\nX_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" X_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_test "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" train_test_split"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" test_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" random_state"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 创建 BaggingClassifier 对象")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("########## Begin ##########")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#行采样，列采样")]),s._v("\nclf "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" BaggingClassifier"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("KNeighborsClassifier"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("max_samples"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" max_features"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("##########  End  ##########")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 调用 fit 函数执行训练过程")]),s._v("\nclf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v("y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# 打印结果")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'训练集准确率：'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" accuracy_score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" clf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X_train"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v("'测试集准确率：'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" accuracy_score"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" clf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X_test"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br"),a("span",{staticClass:"line-number"},[s._v("9")]),a("br"),a("span",{staticClass:"line-number"},[s._v("10")]),a("br"),a("span",{staticClass:"line-number"},[s._v("11")]),a("br"),a("span",{staticClass:"line-number"},[s._v("12")]),a("br"),a("span",{staticClass:"line-number"},[s._v("13")]),a("br"),a("span",{staticClass:"line-number"},[s._v("14")]),a("br"),a("span",{staticClass:"line-number"},[s._v("15")]),a("br"),a("span",{staticClass:"line-number"},[s._v("16")]),a("br"),a("span",{staticClass:"line-number"},[s._v("17")]),a("br"),a("span",{staticClass:"line-number"},[s._v("18")]),a("br"),a("span",{staticClass:"line-number"},[s._v("19")]),a("br"),a("span",{staticClass:"line-number"},[s._v("20")]),a("br"),a("span",{staticClass:"line-number"},[s._v("21")]),a("br"),a("span",{staticClass:"line-number"},[s._v("22")]),a("br"),a("span",{staticClass:"line-number"},[s._v("23")]),a("br"),a("span",{staticClass:"line-number"},[s._v("24")]),a("br"),a("span",{staticClass:"line-number"},[s._v("25")]),a("br"),a("span",{staticClass:"line-number"},[s._v("26")]),a("br"),a("span",{staticClass:"line-number"},[s._v("27")]),a("br"),a("span",{staticClass:"line-number"},[s._v("28")]),a("br"),a("span",{staticClass:"line-number"},[s._v("29")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("BaggingClassifier"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("base_estimator"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" n_estimators"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" max_samples"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" max_features"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bootstrap"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" bootstrap_features"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" oob_score"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" warm_start"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" n_jobs"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" random_state"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[s._v("None")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" verbose"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br")])]),a("p",[s._v("base_estimator : 对象或无，可选（默认=无）\n基本估计量适合数据集的随机子集。如果为None，则基本估计量为决策树。")]),s._v(" "),a("p",[s._v("n_estimators : int，可选（默认值为10）\n集合中的基本估计量。")]),s._v(" "),a("p",[s._v("max_samples : int或float，可选（默认值= 1.0）\n从X抽取以训练每个基本估计量的样本数。")]),s._v(" "),a("p",[s._v("如果为int，则抽取样本 max_samples。\n如果float，则抽取本 max_samples * X.shape[0]")]),s._v(" "),a("p",[s._v("max_features : int或float，可选（默认值= 1.0）\n从X绘制以训练每个基本估计量的要素数量。")]),s._v(" "),a("p",[s._v("如果为int，则绘制特征 max_features。\n如果是浮动的，则绘制特征 max_features * X.shape[1]")]),s._v(" "),a("p",[s._v("bootstrap : 布尔值，可选（默认= True）\n是否抽取样本进行替换。如果为False，则执行不替换的采样。")]),s._v(" "),a("p",[s._v("oob_score : 布尔变量，可选（默认为False）\n是否使用现成的样本来估计泛化误差。")]),s._v(" "),a("p",[s._v("warm_start : 布尔变量，可选（默认= False）\n设置为True时，请重用上一个调用的解决方案以适合并为集合添加更多估计量，否则，仅适合一个全新的集合。")]),s._v(" "),a("p",[s._v("n_jobs : int或None（可选）（默认为None）\nfit和 并行运行的作业数predict。None除非joblib.parallel_backend上下文中，否则表示1 。-1表示使用所有处理器。")]),s._v(" "),a("p",[s._v("random_state : 整型，RandomState实例或无，可选（默认值：无）\n如果为int，则random_state是随机数生成器使用的种子；否则为false。")])])}),[],!1,null,null,null);t.default=e.exports}}]);